{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow-tutorials-14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIeDEE9JV5Pi"
      },
      "source": [
        "# Adding Regularization with l2 and Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wM3jwGUVJxM",
        "outputId": "1fb0d5a2-8422-4559-d369-408b6a307075"
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # for ignoring information messages from tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers   # add hidden layers\n",
        "from tensorflow.keras.datasets import cifar10  # inbuilt datasets\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) =  cifar10.load_data()   # split our mnist dataset in train and test sets and load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0     \n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "    x = layers.Conv2D(32, 3,\n",
        "                      padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 5, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x=  keras.activations.relu(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation='relu' ,\n",
        "                      kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = my_model()\n",
        "model.compile(\n",
        "    # HERE WE ARE TELLING KEARS HOW CONFIGURE THE TRAINING PART OF OUR NETWORKS.\n",
        "    # 1. HERE WE ARE USING SPARSECATEGORICALCROSSENTROPY AND MAKING from_logits=False \n",
        "    # beacuse in our output layers we not apply softmax activation function.\n",
        "    # 2. HERE WE ARE USING ADAM OPTIMIZER WITH 0.001 LEARNING RATE\n",
        "    # 3. FOR EVALUATING WE ARE USING ACCURACY\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=100, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 12s 0us/step\n",
            "170508288/170498071 [==============================] - 12s 0us/step\n",
            "Epoch 1/100\n",
            "782/782 - 41s - loss: 3.0939 - accuracy: 0.1317\n",
            "Epoch 2/100\n",
            "782/782 - 11s - loss: 2.2649 - accuracy: 0.1639\n",
            "Epoch 3/100\n",
            "782/782 - 11s - loss: 2.0999 - accuracy: 0.2207\n",
            "Epoch 4/100\n",
            "782/782 - 11s - loss: 2.0065 - accuracy: 0.2509\n",
            "Epoch 5/100\n",
            "782/782 - 11s - loss: 1.9394 - accuracy: 0.2671\n",
            "Epoch 6/100\n",
            "782/782 - 11s - loss: 1.8892 - accuracy: 0.2838\n",
            "Epoch 7/100\n",
            "782/782 - 11s - loss: 1.8478 - accuracy: 0.2939\n",
            "Epoch 8/100\n",
            "782/782 - 11s - loss: 1.8278 - accuracy: 0.3063\n",
            "Epoch 9/100\n",
            "782/782 - 11s - loss: 1.8002 - accuracy: 0.3214\n",
            "Epoch 10/100\n",
            "782/782 - 11s - loss: 1.7836 - accuracy: 0.3309\n",
            "Epoch 11/100\n",
            "782/782 - 11s - loss: 1.7600 - accuracy: 0.3500\n",
            "Epoch 12/100\n",
            "782/782 - 11s - loss: 1.7374 - accuracy: 0.3629\n",
            "Epoch 13/100\n",
            "782/782 - 11s - loss: 1.7220 - accuracy: 0.3793\n",
            "Epoch 14/100\n",
            "782/782 - 10s - loss: 1.7044 - accuracy: 0.3831\n",
            "Epoch 15/100\n",
            "782/782 - 10s - loss: 1.6958 - accuracy: 0.3926\n",
            "Epoch 16/100\n",
            "782/782 - 10s - loss: 1.6771 - accuracy: 0.3975\n",
            "Epoch 17/100\n",
            "782/782 - 10s - loss: 1.6627 - accuracy: 0.4048\n",
            "Epoch 18/100\n",
            "782/782 - 10s - loss: 1.6640 - accuracy: 0.4036\n",
            "Epoch 19/100\n",
            "782/782 - 10s - loss: 1.6581 - accuracy: 0.4096\n",
            "Epoch 20/100\n",
            "782/782 - 10s - loss: 1.6567 - accuracy: 0.4123\n",
            "Epoch 21/100\n",
            "782/782 - 10s - loss: 1.6489 - accuracy: 0.4152\n",
            "Epoch 22/100\n",
            "782/782 - 10s - loss: 1.6413 - accuracy: 0.4165\n",
            "Epoch 23/100\n",
            "782/782 - 10s - loss: 1.6353 - accuracy: 0.4211\n",
            "Epoch 24/100\n",
            "782/782 - 10s - loss: 1.6400 - accuracy: 0.4255\n",
            "Epoch 25/100\n",
            "782/782 - 10s - loss: 1.6283 - accuracy: 0.4316\n",
            "Epoch 26/100\n",
            "782/782 - 10s - loss: 1.6163 - accuracy: 0.4369\n",
            "Epoch 27/100\n",
            "782/782 - 10s - loss: 1.6151 - accuracy: 0.4386\n",
            "Epoch 28/100\n",
            "782/782 - 10s - loss: 1.6102 - accuracy: 0.4391\n",
            "Epoch 29/100\n",
            "782/782 - 10s - loss: 1.6110 - accuracy: 0.4371\n",
            "Epoch 30/100\n",
            "782/782 - 10s - loss: 1.6021 - accuracy: 0.4404\n",
            "Epoch 31/100\n",
            "782/782 - 10s - loss: 1.5982 - accuracy: 0.4422\n",
            "Epoch 32/100\n",
            "782/782 - 10s - loss: 1.5915 - accuracy: 0.4453\n",
            "Epoch 33/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCrx8f7xV229"
      },
      "source": [
        ""
      ]
    }
  ]
}