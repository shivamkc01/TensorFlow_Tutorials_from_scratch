{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Simple Neural Network**","metadata":{}},{"cell_type":"code","source":" \nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # for ignoring information messages from tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers   # add hidden layers\nfrom tensorflow.keras.datasets import mnist  # inbuilt datasets\nimport warnings\nwarnings.filterwarnings('ignore')\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()   # split our mnist dataset in train and test sets and load_data()\n# print(x_train.shape)  -> (60000, 28, 28)\n# print(y_train.shape)  -> (60000,)\n\n# for sending to the neural network we need to flatten our feature in single(one) column.\n# In below code we are doing\n# 1. flatten the layer by using reshape, -1 was is to keep first value as it is(60000)\n# 2. there going to be in nummpy array also float64 so we change to float32 to make less computational and normalizaing also by dividing 255.0 \nx_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0     # \nx_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0j\nprint(x_train.shape)\nprint(x_test.shape)\n\n# Sequential API (Very convenient, not very flexible)\nmodel = keras.Sequential(\n    [  # HERE WE ARE SENDING THE LIST \n        # 1. FIRST FULLY CONNECTED LAYER (512) NODE\n        # 2. SECOND FULLY CONNECTED LAYERS (256) NODE\n        # 3. OUTPUT LAYER 10 NODE\n        keras.Input(shape=(28 * 28)),\n        layers.Dense(512, activation=\"relu\"),   # here we are applying relu activation function\n        layers.Dense(256, activation=\"relu\"),\n        layers.Dense(10),\n    ]\n)\nprint(model.summary())\n\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(784)))\nmodel.add(layers.Dense(512, activation=\"relu\"))\nmodel.add(layers.Dense(256, activation=\"relu\", name=\"my_layer\"))\nmodel.add(layers.Dense(10))\n\n# Functional API (A bit more flexible)\ninputs = keras.Input(shape=(784))\nx = layers.Dense(512, activation=\"relu\", name=\"first_layer\")(inputs)\nx = layers.Dense(256, activation=\"relu\", name=\"second_layer\")(x)\noutputs = layers.Dense(10, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    # HERE WE ARE TELLING KEARS HOW CONFIGURE THE TRAINING PART OF OUR NETWORKS.\n    # 1. HERE WE ARE USING SPARSECATEGORICALCROSSENTROPY AND MAKING from_logits=False \n    # beacuse in our output layers we not apply softmax activation function.\n    # 2. HERE WE ARE USING ADAM OPTIMIZER WITH 0.001 LEARNING RATE\n    # 3. FOR EVALUATING WE ARE USING ACCURACY\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    optimizer=keras.optimizers.Adam(lr=0.001),\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\nmodel.evaluate(x_test, y_test, batch_size=32, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:06:19.943743Z","iopub.execute_input":"2021-10-08T11:06:19.944223Z","iopub.status.idle":"2021-10-08T11:06:36.490736Z","shell.execute_reply.started":"2021-10-08T11:06:19.944175Z","shell.execute_reply":"2021-10-08T11:06:36.489768Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Suggestions to imporve furture:\n* Try and see what accuracy you can get by increasing the model, training for longer, elecetera. You should be able to get over 98.2% on the test set!\n* Try using different optimizers than Adam, for exapmple graident descent with momontume, adagrad, and RMSprop\n* Is they any difference if you remove the normalization of the data.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}